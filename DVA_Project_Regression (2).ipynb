{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKiGw5VbRQGv"
      },
      "outputs": [],
      "source": [
        "#download from Kaggle\n",
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d jeffsinsel/nyc-fhvhv-data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative method to download data\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jeffsinsel/nyc-fhvhv-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "!mv {path}/* /content"
      ],
      "metadata": {
        "id": "Swv78QZrCNK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SPJjef6xU1TP",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "#unzip files in nyc-fhvhv-data from Kaggle\n",
        "! unzip nyc-fhvhv-data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJpwGZouaL6X"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "import glob\n",
        "\n",
        "# initialize spark\n",
        "spark = SparkSession.builder.appName(\"NYC_Rides\").getOrCreate()\n",
        "\n",
        "# list of all parquet files from content folder\n",
        "parquet_files = glob.glob('/content/*.parquet')\n",
        "\n",
        "removed_summary = {}\n",
        "combined_df = None\n",
        "\n",
        "for file in parquet_files:\n",
        "    # read parquet files get features\n",
        "    df = spark.read.parquet(file).select(\n",
        "        \"Pickup_datetime\", \"DropOff_datetime\", \"PULocationID\", \"DOLocationID\",\n",
        "        \"base_passenger_fare\", \"trip_miles\", \"tips\", \"driver_pay\", \"trip_time\",\n",
        "        \"Hvfhs_license_num\", \"congestion_surcharge\"\n",
        "    )\n",
        "\n",
        "    initial_count = df.count()\n",
        "\n",
        "    # Clean data by filtering out null and invalid values\n",
        "    df_clean = df.filter(\n",
        "        (F.col(\"Pickup_datetime\").isNotNull()) &\n",
        "        (F.col(\"DropOff_datetime\").isNotNull()) &\n",
        "        (F.col(\"PULocationID\").isNotNull()) &\n",
        "        (F.col(\"DOLocationID\").isNotNull()) &\n",
        "        (F.col(\"base_passenger_fare\").isNotNull()) &\n",
        "        (F.col(\"trip_miles\").isNotNull()) &\n",
        "        (F.col(\"tips\").isNotNull()) &\n",
        "        (F.col(\"driver_pay\").isNotNull()) &\n",
        "        (F.col(\"trip_time\").isNotNull()) &\n",
        "        (F.col(\"Hvfhs_license_num\").isNotNull()) &\n",
        "        (F.col(\"congestion_surcharge\").isNotNull()) &\n",
        "\n",
        "\n",
        "        # Ensure numeric columns have valid positive values\n",
        "        (F.col(\"base_passenger_fare\") > 0) &\n",
        "        (F.col(\"trip_miles\") > 0) &\n",
        "        (F.col(\"driver_pay\") > 0) &\n",
        "        (F.col(\"trip_time\") > 0)\n",
        "    )\n",
        "\n",
        "    clean_count = df_clean.count()\n",
        "    removed_summary[file] = initial_count - clean_count\n",
        "\n",
        "    # combine clean DFs into one big DF\n",
        "    if combined_df is None:\n",
        "        combined_df = df_clean\n",
        "    else:\n",
        "        combined_df = combined_df.union(df_clean)\n",
        "\n",
        "# check how many of which file were removed\n",
        "print(\"Removal Summary:\", removed_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_VA9Ju4l4lr"
      },
      "outputs": [],
      "source": [
        "# calculate how many total rows of data were removed\n",
        "tot = 0\n",
        "for key,val in removed_summary.items():\n",
        "  tot += int(val)\n",
        "\n",
        "count = combined_df.count()\n",
        "percent_removed = tot/count * 100\n",
        "\n",
        "print(\"{}% was removed from a total of {} rides in 46 months\".format(percent_removed, count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCTcblxHUWBs",
        "outputId": "ca92afe8-28d8-4dc3-defc-cf705d43984b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.14.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m81.9/85.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZxUPzW78ymN"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import hour\n",
        "\n",
        "# Extract Hour from Pickup_datetime\n",
        "combined_df = combined_df.withColumn(\"hour\", hour(\"Pickup_datetime\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiOTJF734Czl"
      },
      "outputs": [],
      "source": [
        "combined_df.show(5)\n",
        "combined_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6cM6cF-83pc"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, sum, min\n",
        "\n",
        "# Group by Pickup Zone, Hour, and Ride-Hailing Company to Aggregate Driver Pay\n",
        "df_grouped = combined_df.groupBy(\"PULocationID\", \"hour\", \"Hvfhs_license_num\").agg(\n",
        "    sum(\"driver_pay\").alias(\"total_driver_pay\"),\n",
        "    sum(\"tips\").alias(\"total_tips\"),\n",
        "    sum(\"trip_miles\").alias(\"total_miles\"),\n",
        "    sum(\"trip_time\").alias(\"total_time_seconds\"),\n",
        "    sum(\"base_passenger_fare\").alias(\"total_base_fare\"),\n",
        "    min(\"Pickup_datetime\").alias(\"earliest_pickup_time\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9ON6rMF9Pew"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert to Pandas dataframe\n",
        "\n",
        "df = df_grouped.toPandas()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K1COQ3Q5_oZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "df_pandas = df.copy()\n",
        "\n",
        "# Add relevant variables\n",
        "df_pandas[\"earliest_pickup_time\"] = pd.to_datetime(df_pandas[\"earliest_pickup_time\"])\n",
        "df_pandas[\"day_of_week\"] = df_pandas[\"earliest_pickup_time\"].dt.dayofweek\n",
        "df_pandas[\"is_weekend\"] = df_pandas[\"day_of_week\"].isin([5, 6]).astype(int)\n",
        "\n",
        "# Convert trip time from seconds to hours\n",
        "df_pandas[\"total_time_hours\"] = df_pandas[\"total_time_seconds\"] / 3600\n",
        "\n",
        "# Handle division by zero\n",
        "df_pandas[\"total_time_hours\"].replace(0, np.nan, inplace=True)\n",
        "\n",
        "# Compute total driver earnings (including tips)\n",
        "df_pandas[\"total_driver_earnings\"] = df_pandas[\"total_driver_pay\"] + df_pandas[\"total_tips\"]\n",
        "\n",
        "# Compute earnings per hour\n",
        "df_pandas[\"earnings_per_hour\"] = df_pandas[\"total_driver_earnings\"] / df_pandas[\"total_time_hours\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDPePuiBxgUG"
      },
      "outputs": [],
      "source": [
        "df_pandas.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhuz5ng-93Lj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Drop unnecessary columns\n",
        "X = df_pandas.drop(columns=[\"total_driver_earnings\", \"earnings_per_hour\", \"earliest_pickup_time\", \"total_driver_pay\", \"total_time_seconds\"])\n",
        "y = df_pandas[\"earnings_per_hour\"]\n",
        "\n",
        "# Encode PULocationID using target mean encoding\n",
        "pulo_mean = df_pandas.groupby(\"PULocationID\")[\"earnings_per_hour\"].mean()\n",
        "df_pandas[\"PULocationID_encoded\"] = df_pandas[\"PULocationID\"].map(pulo_mean)\n",
        "X = X.drop(columns=[\"PULocationID\"])\n",
        "\n",
        "# One-hot encode Hvfhs_license_num\n",
        "X = pd.get_dummies(X, columns=[\"Hvfhs_license_num\"], drop_first=True)\n",
        "X = X.astype(float)\n",
        "\n",
        "# Fix extreme outliers in target variable\n",
        "y = y.clip(upper=y.quantile(0.99))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHHYfskSlrQA"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAAkLrr-RxE5"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# Train different models to choose the best performing one\n",
        "\n",
        "models = {\n",
        "    \"RandomForest\": RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
        "    \"XGBoost\": XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    results[name] = {\"R² Score\": r2, \"MAE\": mae}\n",
        "\n",
        "# Convert results dictionary to DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "\n",
        "# Display the results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph7CADllGC4S"
      },
      "outputs": [],
      "source": [
        "#GradientBoosting is the best model, therefore I'll use it to predict driver earnings for your heatmap\n",
        "\n",
        "# Train GradientBoosting with the whole dataset\n",
        "best_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
        "best_model.fit(X, y)\n",
        "\n",
        "# Predict earnings_per_hour\n",
        "y_pred = best_model.predict(X)\n",
        "\n",
        "# Attach Predictions to PULocationID\n",
        "df_pandas[\"predicted_earnings_per_hour\"] = y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIFZ2GV6dZvJ"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe with PULocationID and predicted_earnings_per_hour for heatmap\n",
        "\n",
        "# Group by PULocationID and Hour to Get Mean Earnings Per Hour\n",
        "earnings_by_location_time = (\n",
        "    df_pandas.groupby([\"PULocationID\", \"hour\"])[\"predicted_earnings_per_hour\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Display first few rows of Results\n",
        "print(earnings_by_location_time.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajrsJFn4o6U4"
      },
      "outputs": [],
      "source": [
        "# Regression for NYC Revenue\n",
        "\n",
        "\n",
        "# creating new copy of dataframe\n",
        "reg2 = df_pandas.copy()\n",
        "reg2['year'] = reg2['earliest_pickup_time'].dt.year\n",
        "\n",
        "# calculating city revenue column by hand\n",
        "# this was the original method\n",
        "reg2['city_revenue'] = reg2['total_base_fare'] * 0.0875\n",
        "reg2['city_revenue'] = reg2['city_revenue'] + np.where(reg2['year'] == 2024, 4.25, 2.75)\n",
        "\n",
        "\n",
        "# getting rid of unccessary columns and creating x&y variables\n",
        "x = reg2.drop(columns=['predicted_earnings_per_hour','total_time_seconds', 'total_driver_pay', 'city_revenue', 'total_tips', 'is_weekend', 'total_driver_earnings', 'earliest_pickup_time', 'total_base_fare', 'PULocationID', 'congestion_surcharge'])\n",
        "x = pd.get_dummies(x, columns=[\"Hvfhs_license_num\"], drop_first=True)\n",
        "print(x.columns)\n",
        "y = reg2['congestion_surcharge'] + reg2['total_base_fare'] * 0.0875\n",
        "y = y.clip(upper=y.quantile(0.99))\n",
        "\n",
        "\n",
        "# run model\n",
        "city_revenue_model = GradientBoostingRegressor(n_estimators=100, learning_rate = 0.1, max_depth=6, random_state=42)\n",
        "city_revenue_model.fit(x,y)\n",
        "\n",
        "prediction = city_revenue_model.predict(x)\n",
        "reg2['predicted_city_revenue_per_hour'] = prediction\n",
        "\n",
        "# create dataframe for city revenue per hour\n",
        "city_revenue = (reg2.groupby(['PULocationID','hour'])['predicted_city_revenue_per_hour'].mean().round(3).reset_index())\n",
        "city_revenue.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pivot column to also visualize data\n",
        "revenue_pivot = city_revenue.pivot(index='PULocationID', columns='hour', values='predicted_city_revenue_per_hour')\n",
        "revenue_pivot"
      ],
      "metadata": {
        "id": "QzJvgnpoqZ52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8qMYgLnQ64Uk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
