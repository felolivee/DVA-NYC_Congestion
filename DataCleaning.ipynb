{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felolivee/DVA-NYC_Congestion/blob/main/DataCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "6v9c4Yqtox0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 1:"
      ],
      "metadata": {
        "id": "Usuxh3HPo49Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tHM8Nh-KtReH"
      },
      "outputs": [],
      "source": [
        "#download from Kaggle\n",
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d jeffsinsel/nyc-fhvhv-data\n",
        "\n",
        "#unzip files in nyc-fhvhv-data from Kaggle to content folder\n",
        "! unzip nyc-fhvhv-data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 2:"
      ],
      "metadata": {
        "id": "lVfWHLgFo-1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install -q kagglehub\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"jeffsinsel/nyc-fhvhv-data\")\n",
        "print(\"Path to dataset:\", path)\n",
        "\n",
        "# move dataset to content folder\n",
        "!mv {path}/* /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqnuuQszd0oW",
        "outputId": "4b267f1a-18dc-455c-f2d8-73bc1b2e1e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jeffsinsel/nyc-fhvhv-data?dataset_version_number=4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17.8G/17.8G [03:33<00:00, 89.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset: /root/.cache/kagglehub/datasets/jeffsinsel/nyc-fhvhv-data/versions/4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Features"
      ],
      "metadata": {
        "id": "dtAAAa8Ithl7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYhZqUvntReI"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "import glob\n",
        "\n",
        "# initialize spark\n",
        "spark = SparkSession.builder.appName(\"NYC_Rides\").getOrCreate()\n",
        "\n",
        "# list of all parquet files from content folder\n",
        "parquet_files = glob.glob('/content/*.parquet')\n",
        "\n",
        "removed_summary = {}\n",
        "combined_df = None\n",
        "\n",
        "for file in parquet_files:\n",
        "    # read parquet files get features\n",
        "    df = spark.read.parquet(file).select(\n",
        "        \"Pickup_datetime\", \"DropOff_datetime\", \"PULocationID\", \"DOLocationID\",\n",
        "        \"base_passenger_fare\", \"trip_miles\", \"tips\", \"driver_pay\", \"trip_time\",\n",
        "        \"Hvfhs_license_num\", \"congestion_surcharge\"\n",
        "    )\n",
        "\n",
        "    initial_count = df.count()\n",
        "\n",
        "    # Clean data by filtering out null and invalid values\n",
        "    df_clean = df.filter(\n",
        "        (F.col(\"Pickup_datetime\").isNotNull()) &\n",
        "        (F.col(\"DropOff_datetime\").isNotNull()) &\n",
        "        (F.col(\"PULocationID\").isNotNull()) &\n",
        "        (F.col(\"DOLocationID\").isNotNull()) &\n",
        "        (F.col(\"base_passenger_fare\").isNotNull()) &\n",
        "        (F.col(\"trip_miles\").isNotNull()) &\n",
        "        (F.col(\"tips\").isNotNull()) &\n",
        "        (F.col(\"driver_pay\").isNotNull()) &\n",
        "        (F.col(\"trip_time\").isNotNull()) &\n",
        "        (F.col(\"Hvfhs_license_num\").isNotNull()) &\n",
        "        (F.col(\"congestion_surcharge\").isNotNull()) &\n",
        "\n",
        "        # Ensure numeric columns have valid positive values\n",
        "        (F.col(\"base_passenger_fare\") > 0) &\n",
        "        (F.col(\"trip_miles\") > 0) &\n",
        "        (F.col(\"driver_pay\") > 0) &\n",
        "        (F.col(\"trip_time\") > 0)\n",
        "    )\n",
        "\n",
        "    clean_count = df_clean.count()\n",
        "    removed_summary[file] = initial_count - clean_count\n",
        "\n",
        "    # combine clean DFs into one big DF\n",
        "    if combined_df is None:\n",
        "        combined_df = df_clean\n",
        "    else:\n",
        "        combined_df = combined_df.union(df_clean)\n",
        "\n",
        "# check how many of which file were removed\n",
        "print(\"Removal Summary:\", removed_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional - See how many rows were removed"
      ],
      "metadata": {
        "id": "Mx2EXB4ctobB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVGS6gdItReJ"
      },
      "outputs": [],
      "source": [
        "# calculate how many total rows of data were removed\n",
        "tot = 0\n",
        "for key,val in removed_summary.items():\n",
        "  tot += int(val)\n",
        "\n",
        "count = combined_df.count()\n",
        "percent_removed = tot/count * 100\n",
        "\n",
        "print(\"{}% was removed from a total of {} rides in 46 months\".format(percent_removed, count))"
      ]
    }
  ]
}